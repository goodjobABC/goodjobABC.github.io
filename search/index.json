[{"content":"如何正确的与AI沟通\r在接触AI的时间里，我也积累了不少与AI沟通的经验，特此记录。\n如有错误，请评论区指正，非常感谢！——Shanziyi\n1.AI生成的图片/文章不合你意？\r在与AI沟通时，你需要把自己的需求用多个关键词描述清楚，举个栗（例）子：\n改善前： 用户：请帮我生成一篇关于人工智能发展史的文章（描述简单，并不清晰）\nAI:好的。 （生成中）……\n最后的生成结果要么是太复杂/太简单，就是描述跑题，意思不当。\n那该怎么办呢？请看下文：\n改善后： 用户：请帮我生成一篇关于人工智能发展史的文章，700~800字左右，特别关注1950~2000年代，以幽默恢侃的语言讲述（描述细致，目标清晰）\nAI:好的。（生成中）……\n最后的生成结果大概率能满足你的要求（可能还会超乎预料）。\n注：使用生成式人工智能生成图片/其他类型的作品时同理\n（本文持续更新中，欢迎你的补充）\n作者：Shanziyi\n本文纯属自己撰稿，转载时请标明出处！\n©Shanziyi 2025\n本文采用CC BY-NC-SA 4.0协议进行署名——Shanziyi\n","date":"2025-02-13T14:53:33+08:00","permalink":"https://goodjobabc.github.io/p/%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E7%9A%84%E4%BD%BF%E7%94%A8ai1/","title":"如何正确的使用AI(1)"},{"content":"安装Ollama与部署DeepSeek：新手必看问题指南\rWindows平台会在接下来的文章中提到\n在学习的时候配上轻音乐吧！\r一、安装Ollama\r安装Ollama 安装Ollama通常需要使用与项目相关的包管理器，如poetry或pip。以下是如何在macOS和Linux上安装Ollama的分步指南：\nmacOS： 检查是否有Python和poetry已安装。如果没有，请先安装Python。\n下载并解压Ollama的仓库：curl -L https://github.com/ggerganov/ollama/releases/download/v3.2.0/ollama-m1.pkg \u0026gt; ollama-m1.pkg; xcrun bin briskt ollama -v.\n使用poetry安装依赖：poetry install --strict.\n启动Ollama客户端：poetry run ollama-client.\nLinux： 确保Python和poetry已安装。如果没有，请安装poetry：sudo apt-get install poetry.\n下载并解压Ollama的仓库：curl -L https://github.com/ggerganov/ollama/releases/download/v3.2.0/ollama Puppy.\n使用poetry安装依赖：poetry install --strict.\n启动Ollama客户端：poetry run ollama-client.\n常见问题\n依赖安装失败：如果依赖安装失败，可能是缺少某些系统库或显卡驱动。请检查并安装相关的系统库，或者使用社区提供的镜像。 编译失败：如果出现编译错误，通常是因为环境变量配置错误。请确保.bash_profile或.zshrc文件中添加了OllamaBgPort相关的配置。 配置问题：如果无法访问Ollama服务，可能是配置文件路径错误。请检查ollama.conf文件，确保路径正确。 二、部署DeepSeek\r部署DeepSeek 部署DeepSeek需要在本地运行一个服务，供Ollama和客户端使用。以下是部署DeepSeek的分步指南：\n安装DeepSeek： 在终端中输入以下命令安装DeepSeek框架：poetry install --strict. 启动DeepSeek服务：poetry run deepseek. 配置环境变量： 在终端中输入以下命令打开~/.deepseek/config文件。 添加以下内容： 1 2 API_KEY=\u0026#34;your_api_key_here\u0026#34; API_BASE_URL=\u0026#34;http://localhost:8080\u0026#34; API_KEY是DeepSeek的API密钥。 API_BASE_URL是DeepSeek的服务器地址。 保存文件并退出。 启动DeepSeek服务： 在终端中输入以下命令启动DeepSeek服务：poetry run deepseek. 检查服务是否正常运行：打开浏览器，访问http://localhost:8080 。 访问DeepSeek服务： 访问http://localhost:8080 。\n在页面找到DeepSeek链接。\n进入DeepSeek页面，输入您的API密钥并开始查询。\n常见问题\nAPI密钥配置错误：如果无法使用DeepSeek，可能是API密钥配置错误。请确保密钥正确，并且格式为API_KEY=\u0026quot;your_api_key_here\u0026quot;，其中your_api_key_here是您从DeepSeek提供的API密钥。 服务启动失败：如果服务无法启动，可能是网络问题或配置文件路径错误。请检查网络状态，或者在~/.deepseek/config文件中添加LogLevel=2以获取更多日志信息。 访问问题：如果无法访问DeepSeek服务，可能是防火墙设置或端口错误。请检查~/.deepseek/config文件中的API_BASE_URL，确保端口和地址设置正确。 总结\r安装Ollama和部署DeepSeek可能需要一些尝试和错误，但通过逐步配置和环境变量调整，这些问题可以迎刃而解。新手在安装过程中遇到问题时，可以参考官方文档或社区资源，逐步优化配置，最终成功搭建本地的Ollama环境。\n完\n作者:Shanziyi © Shanziyi 2025 本文采用CC BY-NC-SA 4.0协议进行署名——Shanziyi\n本文纯属自己撰稿，转载时请标明出处 ！\n","date":"2025-02-12T14:06:09+08:00","permalink":"https://goodjobabc.github.io/p/%E9%83%A8%E7%BD%B2deepseek%E5%A4%B1%E8%B4%A5%E6%96%B0%E6%89%8B%E9%97%AE%E9%A2%98%E6%8C%87%E5%8D%972/","title":"部署deepseek失败？新手问题指南2"},{"content":" 观前提示：\n本篇问答相对专业一些，如果仅是自己在本地尝试运行ollma时的基本问题，请期待下一篇问答文章。\n在问答的时候配上轻音乐吧！\n问题：1. DeepSeek 模型如何转换为 Ollama 相容的格式？\r答案：\r要将 DeepSeek 模型部署到 Ollama，需要先将模型转换为 Ollama 支持的格式。以下是具体步骤：\n选择适当的模型大小： 根据 Ollama 的架构，选择适合的模型大小（如 7B、13B 等）。\n大模型可能需要更多内存和计算资源。\n模型转换工具： 使用工具如 llama.cpp 或其他开源工具将 DeepSeek 模型转换为 Ollama 相容的格式。\n例如：\n1 llama-convert --source deepseek --target ollama --n 4 --fp16 生成的文件（如 ollama.f16）是 Ollama 运行所需的参数文件。 问题：2. 为什么转换后的模型在 Ollama 中加载失败？\r答案：\r如果转换后的模型在 Ollama 中加载失败，可能有以下原因：\n模型转换错误： 检查转换工具的日志，确保转换过程没有错误。 确保 DeepSeek 模型路径正确。 依赖项配置： 确保所有依赖项（如 llama.cpp、llama-convert 等）已正确安装并配置。 检查 Makefile 配置，确保环境变量正确设置（如 O_LLLMABackend=1）。 缺少必要的头文件： 确保 o2c 头文件路径正确，例如： 1 #include \u0026lt;o2c/o2c.h\u0026gt; 问题：3. 如何配置 Ollama 实例以支持多 GPU？\r答案：\r本问题演示环境为linux系统\n要使 Ollama 支持多 GPU，需配置 Ollama 实例为多 GPU 模式：\n实例选择： 使用支持多 GPU 的实例类型（如 AWS EC2 多 GPU 实例、Azure VM 多 GPU 实例）。 配置文件： 在 /root/.ollama/config/params.json 中设置多 GPU 配置： 1 2 3 4 { \u0026#34;num GPUs\u0026#34;: 2, \u0026#34;multi-GPU backend\u0026#34;: \u0026#34;cudai\u0026#34; } 环境变量： 置 LD_LIBRARY_PATH 包含 GPU 驱动程序（如 CUDA 兼容的驱动）。 验证配置： 运行ollama --version，确认支持多 GPU。 问题：4. 如何将 DeepSeek 模型部署到 Ollama 并连接到云平台？\r答案：\r部署 DeepSeek 模型到 Ollama 并连接到云平台的步骤如下：\n部署 Ollama： 使用云平台（如 AWS、GCP、Azure）部署 Ollama 实例。 指定 GPU 实例以支持 DeepSeek 模型的计算需求。 模型转换和加载： 使用工具将 DeepSeek 模型转换为 Ollama 相容格式。 在 Ollama 实例中加载转换后的模型。 连接到云平台： 使用云平台提供的 SDK（如 AWS SDK、GCP SDK）连接到 Ollama 实例。 发送推理请求，运行 DeepSeek 模型。 问题：5. 在 Ollama 中运行 DeepSeek 模型时遇到性能问题，该如何优化？\r答案：\r如果在 Ollama 中运行 DeepSeek 模型时遇到性能问题，可以尝试以下优化方法：\n模型量化： 使用量化技术（如 FP16 或 INT8）减少模型大小和内存占用。 化转换工具的量化设置。 实例配置： 根据模型性能需求，选择适当的实例大小（如 T4、V100 等 GPU）。 调整多 GPU 实例的负载均衡策略。 优化代码： 使用优化的 Ollama 库或框架（如 o2c 或 ollama-cpp）。 编写高效的推理代码，减少 I/O 和通信开销。 监控和调整： 使用云平台的监控工具（如 AWS CloudWatch、GCP Datadog）监控 Ollama 实例的性能。 根据监控结果逐步优化模型大小、实例配置和推理代码。 总结：\r部署 DeepSeek 到 Ollama 涉及模型转换、实例配置和性能优化等多个环节。确保每一步都按照正确的步骤执行，并根据具体情况调整配置，以最大化性能和效率。遇到问题时，参考官方文档和社区资源，或联系技术支持。\n完\n作者: Shanziyi 本文纯属自己撰稿，无具体参考资料 ©Copyright 2025 Shanziyi\n本文采用CC BY-NC-SA 4.0协议进行署名——Shanziyi\n","date":"2025-02-11T09:42:30+08:00","permalink":"https://goodjobabc.github.io/p/%E5%9C%A8%E5%B0%86-deepseek-%E9%83%A8%E7%BD%B2%E5%88%B0-ollama-%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98faq/","title":"在将 DeepSeek 部署到 Ollama 的过程中遇到的问题（FAQ）"},{"content":"在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南\r自2025/2/14起，本文已采用 copyright协议，禁止转载/商业使用——作者。\n在学习的时候配上纯音乐吧！\n嗯，最近我在学习如何在本地部署模型，想着能不能把 DeepSeek 搬到 Ollama 里，这样用起来应该挺方便的吧！作为一个新手，我决定写一篇文章分享我的探索过程，希望能帮到像我一样的小白。\n准备阶段：安装好装备\r首先，我需要准备好几个“装备”，这些装备是部署 DeepSeek 的必要条件：\n操作系统：当然是需要一台能流畅运行的电脑，Windows、Mac 或者 Linux 都可以。 硬件：显卡、内存、硬盘空间都需要足够的容量。\n编程环境：我选择了 Python，毕竟机器学习离不开它。所以，我得先安装 Python 和一些必要的依赖库，比如 numpy 和 pandas。这些库可以通过 pip 安装：\n1 pip install numpy pandas 深度学习框架：为了训练或部署模型，我需要 PyTorch 或 TensorFlow。这里我选择了 PyTorch，因为它感觉更友好一些。安装命令是：\n1 pip install torch torchvision 下载 DeepSeek 模型：资源准备\n接下来，我需要下载 DeepSeek 的模型文件。这个模型可是个大家伙，下载的时候要注意：\n选择模型大小：\nDeepSeek 有小有大，小版本适合在本地测试，大版本则适合实际应用。我选择了中等大小的模型，大约 20GB 左右。 下载速度：别问我为什么，就是喜欢快一点。所以我选择了网速好的时间下载，大概花了我一个晚上。 存储位置：下载完成后，我不会让它孤零零地躺在硬盘里，而是用 mkdir -p models/deepseek 创建了一个专门的目录，方便以后管理。（推荐这么做）\n架构搭建：本地服务器搭建 现在，我需要搭建一个本地服务器，让它能处理 DeepSeek 的请求。这里有两个选择：本地服务框架和 Ollama。我选择了 Ollama，因为它简单易用，而且我也不太想写复杂的 API。\n安装 Ollama: 首先，我需要安装 Ollama。这是一个轻量级的 API，支持多种语言。安装命令是：\n1 2 3 git clone --recursive https://github.com/ollama/ollama-stable cd ollama-stable python setup.py install 配置 Ollama：安装完成后，我需要告诉 Ollama 位置在哪里。这一步可以通过 ollama config 设置：\n1 2 ollama config set-model-path models/deepseek ollama config set-inference-batch-size 1 这样一来，Ollama 就知道我的模型在哪里了。\n搭配 DeepSeek：模型配置\r现在，我需要让 DeepSeek 和 Ollama 配对，这样才能协作。这一步有点挑战\n设置 DeepSeek 配置文件：DeepSeek 的配置文件位于 models/deepseek/params.json 或 .toml 或 .yaml。我需要确保这个文件的路径正确。如果路径不对，DeepSeek 和 Ollama 就无法通信了。 启动 DeepSeek：运行 DeepSeek 后，我需要告诉它 Ollama 的服务端口。通常，Ollama 服务监听在 1234 端口。所以，我添加了以下配置：\n1 2 3 4 5 [model] name = deepseek basedir = models/deepseek endpoint = ollama:1234 inference-batch-size = 1 测试阶段：验证部署\r终于到了测试环节，我需要确保部署成功。步骤如下：\n运行 Ollama 服务：启动 Ollama 服务，确保它可以正常工作。打开终端： 1 python -m ollama ollama-stable/ollama 如果服务启动成功，Ollama 会输出一条欢快的的信息。\n访问 DeepSeek 端点：打开浏览器，进入 http://localhost:8080 。这是我配置的 Default 服务端口。\n发送预测请求：输入一些简单的文本，比如“Hello, world!”，然后按回车。Ollama 应该会将请求转发给 DeepSeek，然后返回预测结果。\n成功时刻：部署完成\r当我看到 DeepSeek 和 Ollama 顺利通信时，内心充满了喜悦。虽然整个过程有点曲折，但最终的结果让我感到成就感满满。\n总结\r部署 DeepSeek 到本地 Ollama 的过程虽然有点复杂，但通过一步步的尝试，我成功了！这让我更加了解了机器学习模型的部署流程，也让我对 Ollama 这个平台有了更深的了解。\n希望这篇文章能帮助到和我一样对部署模型感到困惑的小白。\n完\n作者：Shanziyi\n本文纯属自己撰稿，禁止转载，引用时请标明出处 ！\n© Shanziyi 2025\n彩蛋：deepseek也参与到了撰稿中！\n","date":"2025-02-09T11:00:19+08:00","permalink":"https://goodjobabc.github.io/p/%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%84%89%E5%BF%AB%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-%E5%88%B0-ollama%E4%B8%80%E4%B8%AA%E8%BD%BB%E6%9D%BE%E6%8C%87%E5%8D%97/","title":"在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南"},{"content":"祝贺本站被谷歌收编！\r2024.12.9，本站正式被谷歌SEO收录！！！以后就可以在谷歌搜索上搜到本站了！\n注意：在使用（谷歌）搜索时，请使用以下关键词：\n\u0026ldquo;goodjobabc.github.io\u0026rdquo;,\u0026ldquo;goodjobabc\u0026rdquo;,\u0026ldquo;github.io\u0026rdquo;\n","date":"2024-12-22T15:50:06+08:00","permalink":"https://goodjobabc.github.io/p/congratulations/","title":"Congratulations"},{"content":"怎样建立属于你的个人博客\r在欣赏博客时配上纯音乐吧~\n需求配置： 注：本文使用霞鹜文楷 声明：本文由@Shanziyi原创，转载/引用请标明出处！ ©Shanziyi 2024 装有Git的Windows PC 一台（Windows 10+） 一个Github账号\n准备工作：\r首先,建立一个仓库，命名必须符合\u0026lt;用户名\u0026gt;.github.io形式，设为公开 接着，建立一个博客文件夹，如D:\\blog\n安装hugo\r法一：使用winget\n1 winget install Hugo.Hugo.Extended 法二：使用包管理器Chocolatey\n安装Chocolatey:\n1 Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://community.chocolatey.org/install.ps1\u0026#39;)) 或参见Chocolatey官网\n安装hugo:\n1 choco install hugo-extended 新建博客项目\r1 hugo new site exampleblog # 可以替换成任意你想要的名字 结构如下图\n接着，在此处右键，选择更多选项···\n选择Git Bash here或Open Git Bash here\n接着，输入命令\n1 2 git init #先输入这条 git submodule \u0026lt; 你想要的主题的仓库网址 \u0026gt; /themes/ \u0026lt; 你的主题名字 \u0026gt; 参考文献（按对本文做的贡献排名）：\n使用 Hugo + Github Pages 部署个人博客——ratmomo\u0026rsquo;s blog Hugo Documentation 解决Hugo无法加载css文件——瞬间的博客 Chocolatey - Software Management for Windows 许可协议及署名： 本文采用CC BY-NC-SA 4.0协议进行署名——Shanziyi\n©Shanziyi 2024\n","date":"2024-12-06T21:24:00+08:00","permalink":"https://goodjobabc.github.io/p/%E6%80%8E%E6%A0%B7%E5%BB%BA%E7%AB%8B%E5%B1%9E%E4%BA%8E%E4%BD%A0%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%8C%85%E5%90%AB%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/","title":"怎样建立属于你的个人博客（包含避坑指南）"},{"content":"单单工作室的博客\r以后关于单单工作室的博客都会在这发布 前往官网\n以下是网站的重要日志，感谢大家支持\r2025/2/11/20:30 | 因为技术问题，网站的文章访问量在2025/2/11/20:30重置（已恢复）\r以下是截至当时各文章的真实访问量：\n在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南：580次\n怎样建立属于你的个人博客：50次\n剩下的皆为十几次左右。\n发布者： Shanziyi\n","date":"2024-12-06T21:14:23+08:00","permalink":"https://goodjobabc.github.io/p/%E4%BB%8B%E7%BB%8D/","title":"介绍"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://goodjobabc.github.io/p/","title":""}]