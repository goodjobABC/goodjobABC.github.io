<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>人工智能 on 单单工作室的博客</title>
        <link>https://goodjobabc.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
        <description>Recent content in 人工智能 on 单单工作室的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>ShanShanStdio</copyright>
        <lastBuildDate>Sun, 09 Feb 2025 11:00:19 +0800</lastBuildDate><atom:link href="https://goodjobabc.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南</title>
        <link>https://goodjobabc.github.io/p/%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%84%89%E5%BF%AB%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-%E5%88%B0-ollama%E4%B8%80%E4%B8%AA%E8%BD%BB%E6%9D%BE%E6%8C%87%E5%8D%97/</link>
        <pubDate>Sun, 09 Feb 2025 11:00:19 +0800</pubDate>
        
        <guid>https://goodjobabc.github.io/p/%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%84%89%E5%BF%AB%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-%E5%88%B0-ollama%E4%B8%80%E4%B8%AA%E8%BD%BB%E6%9D%BE%E6%8C%87%E5%8D%97/</guid>
        <description>&lt;h1 id=&#34;在本地愉快地部署-deepseek-到-ollama一个轻松指南&#34;&gt;在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;在学习的时候配上纯音乐吧！&lt;/strong&gt;&lt;/p&gt;
&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=30987703&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;嗯，最近我在学习如何在本地部署模型，想着能不能把 DeepSeek 搬到 Ollama 里，这样用起来应该挺方便的吧！作为一个新手，我决定写一篇文章分享我的探索过程，希望能帮到像我一样的小白。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;准备阶段安装好装备&#34;&gt;准备阶段：安装好装备
&lt;/h2&gt;&lt;p&gt;首先，我需要准备好几个“装备”，这些装备是部署 DeepSeek 的必要条件：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;操作系统：当然是需要一台能流畅运行的电脑，Windows、Mac 或者 Linux 都可以。
硬件：显卡、内存、硬盘空间都需要足够的容量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编程环境：我选择了 Python，毕竟机器学习离不开它。所以，我得先安装 Python 和一些必要的依赖库，比如 numpy 和 pandas。这些库可以通过 pip 安装：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install numpy pandas
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;深度学习框架&lt;/b&gt;：为了训练或部署模型，我需要 PyTorch 或 TensorFlow。这里我选择了 PyTorch，因为它感觉更友好一些。安装命令是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install torch torchvision
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;下载 DeepSeek 模型：资源准备&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;接下来，我需要下载 DeepSeek 的模型文件。这个模型可是个大家伙，下载的时候要注意：&lt;/p&gt;
&lt;p&gt;&lt;b&gt;选择模型大小：&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;DeepSeek 有小有大，小版本适合在本地测试，大版本则适合实际应用。我选择了中等大小的模型，大约 20GB 左右。
下载速度：别问我为什么，就是喜欢快一点。所以我选择了网速好的时间下载，大概花了我一个晚上。
存储位置：下载完成后，我不会让它孤零零地躺在硬盘里，而是用 &lt;code&gt;mkdir -p models/deepseek&lt;/code&gt; 创建了一个专门的目录，方便以后管理。&lt;b&gt;（推荐这么做）&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;架构搭建：本地服务器搭建&lt;/b&gt;
现在，我需要搭建一个本地服务器，让它能处理 DeepSeek 的请求。这里有两个选择：本地服务框架和 Ollama。我选择了 Ollama，因为它简单易用，而且我也不太想写复杂的 API。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安装 Ollama&lt;/strong&gt;: 首先，我需要安装 Ollama。这是一个轻量级的 API，支持多种语言。安装命令是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone --recursive https://github.com/ollama/ollama-stable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ollama-stable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python setup.py install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;配置 Ollama：&lt;/b&gt;安装完成后，我需要告诉 Ollama 位置在哪里。这一步可以通过 ollama config 设置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama config set-model-path models/deepseek
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama config set-inference-batch-size &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这样一来，Ollama 就知道我的模型在哪里了。&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;搭配-deepseek模型配置&#34;&gt;搭配 DeepSeek：模型配置
&lt;/h4&gt;&lt;p&gt;现在，我需要让 DeepSeek 和 Ollama 配对，这样才能协作。这一步有点挑战&lt;/p&gt;
&lt;p&gt;设置 DeepSeek 配置文件：DeepSeek 的配置文件位于 &lt;code&gt;models/deepseek/params.json 或 .toml 或 .yaml&lt;/code&gt;。我需要确保这个文件的路径正确。如果路径不对，DeepSeek 和 Ollama 就无法通信了。
启动 DeepSeek：运行 DeepSeek 后，我需要告诉它 Ollama 的服务端口。通常，Ollama 服务监听在 1234 端口。所以，我添加了以下配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;deepseek&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;basedir&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;models&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;deepseek&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;endpoint&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;inference-batch-size&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;测试阶段验证部署&#34;&gt;测试阶段：验证部署
&lt;/h4&gt;&lt;p&gt;终于到了测试环节，我需要确保部署成功。步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;运行 Ollama 服务：启动 Ollama 服务，确保它可以正常工作。打开终端：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m ollama ollama-stable/ollama
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;如果服务启动成功，Ollama 会输出一条欢快的的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;访问 DeepSeek 端点：打开浏览器，进入 http://localhost:8080 。这是我配置的 Default 服务端口。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发送预测请求：输入一些简单的文本，比如“Hello, world!”，然后按回车。Ollama 应该会将请求转发给 DeepSeek，然后返回预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;成功时刻部署完成&#34;&gt;成功时刻：部署完成
&lt;/h4&gt;&lt;p&gt;当我看到 DeepSeek 和 Ollama 顺利通信时，内心充满了喜悦。虽然整个过程有点曲折，但最终的结果让我感到成就感满满。&lt;/p&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结
&lt;/h4&gt;&lt;p&gt;部署 DeepSeek 到本地 Ollama 的过程虽然有点复杂，但通过一步步的尝试，我成功了！这让我更加了解了机器学习模型的部署流程，也让我对 Ollama 这个平台有了更深的了解。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;希望这篇文章能帮助到和我一样对部署模型感到困惑的小白。记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;big&gt;完&lt;/big&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：Shanziyi&lt;br&gt;
本文纯属自己撰稿，转载时请标明出处 ！&lt;br&gt;
© Shanziyi 2025&lt;br&gt;
彩蛋：deepseek也参与到了撰稿中！&lt;br&gt;
&lt;strong&gt;本文采用&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA 4.0协议&lt;/a&gt;进行署名——Shanziyi&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>
