<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>人工智能 on 单单工作室的博客</title>
        <link>https://goodjobabc.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
        <description>Recent content in 人工智能 on 单单工作室的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>ShanShanStdio</copyright>
        <lastBuildDate>Tue, 11 Feb 2025 09:42:30 +0800</lastBuildDate><atom:link href="https://goodjobabc.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>在将 DeepSeek 部署到 Ollama 的过程中遇到的问题（FAQ）</title>
        <link>https://goodjobabc.github.io/p/%E5%9C%A8%E5%B0%86-deepseek-%E9%83%A8%E7%BD%B2%E5%88%B0-ollama-%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98faq/</link>
        <pubDate>Tue, 11 Feb 2025 09:42:30 +0800</pubDate>
        
        <guid>https://goodjobabc.github.io/p/%E5%9C%A8%E5%B0%86-deepseek-%E9%83%A8%E7%BD%B2%E5%88%B0-ollama-%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98faq/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;观前提示：&lt;br&gt;
本篇问答相对专业一些，如果仅是自己在本地尝试运行ollma时的基本问题，请期待下一篇问答文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;在问答的时候配上轻音乐吧！&lt;/strong&gt;&lt;/p&gt;
&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=41659405&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;
&lt;h4 id=&#34;问题1-deepseek-模型如何转换为-ollama-相容的格式&#34;&gt;问题：1. DeepSeek 模型如何转换为 Ollama 相容的格式？
&lt;/h4&gt;&lt;h4 id=&#34;答案&#34;&gt;答案：
&lt;/h4&gt;&lt;p&gt;要将 DeepSeek 模型部署到 Ollama，需要先将模型转换为 Ollama 支持的格式。以下是具体步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择适当的模型大小：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;根据 Ollama 的架构，选择适合的模型大小（如 7B、13B 等）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;大模型可能需要更多内存和计算资源。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;模型转换工具：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用工具如 llama.cpp 或其他开源工具将 DeepSeek 模型转换为 Ollama 相容的格式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;llama-convert --source deepseek --target ollama --n &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; --fp16
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;生成的文件（如 ollama.f16）是 Ollama 运行所需的参数文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;问题2-为什么转换后的模型在-ollama-中加载失败&#34;&gt;问题：2. 为什么转换后的模型在 Ollama 中加载失败？
&lt;/h4&gt;&lt;h4 id=&#34;答案-1&#34;&gt;答案：
&lt;/h4&gt;&lt;p&gt;如果转换后的模型在 Ollama 中加载失败，可能有以下原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型转换错误：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;检查转换工具的日志，确保转换过程没有错误。&lt;/li&gt;
&lt;li&gt;确保 DeepSeek 模型路径正确。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;依赖项配置：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;确保所有依赖项（如 llama.cpp、llama-convert 等）已正确安装并配置。&lt;/li&gt;
&lt;li&gt;检查 Makefile 配置，确保环境变量正确设置（如 O_LLLMABackend=1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;缺少必要的头文件：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;确保 o2c 头文件路径正确，例如：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;o2c/o2c.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;hr&gt;
&lt;h4 id=&#34;问题3-如何配置-ollama-实例以支持多-gpu&#34;&gt;问题：3. 如何配置 Ollama 实例以支持多 GPU？
&lt;/h4&gt;&lt;h4 id=&#34;答案-2&#34;&gt;答案：
&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;本问题演示环境为linux系统&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;要使 Ollama 支持多 GPU，需配置 Ollama 实例为多 GPU 模式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实例选择：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用支持多 GPU 的实例类型（如 AWS EC2 多 GPU 实例、Azure VM 多 GPU 实例）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;配置文件：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;/root/.ollama/config/params.json&lt;/code&gt; 中设置多 GPU 配置：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;num GPUs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;multi-GPU backend&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;cudai&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;环境变量：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;置 LD_LIBRARY_PATH 包含 GPU 驱动程序（如 CUDA 兼容的驱动）。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;验证配置：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;运行&lt;code&gt;ollama --version&lt;/code&gt;，确认支持多 GPU。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;问题4-如何将-deepseek-模型部署到-ollama-并连接到云平台&#34;&gt;问题：4. 如何将 DeepSeek 模型部署到 Ollama 并连接到云平台？
&lt;/h4&gt;&lt;h4 id=&#34;答案-3&#34;&gt;答案：
&lt;/h4&gt;&lt;p&gt;部署 DeepSeek 模型到 Ollama 并连接到云平台的步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;部署 Ollama：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用云平台（如 AWS、GCP、Azure）部署 Ollama 实例。&lt;/li&gt;
&lt;li&gt;指定 GPU 实例以支持 DeepSeek 模型的计算需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;模型转换和加载：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用工具将 DeepSeek 模型转换为 Ollama 相容格式。&lt;/li&gt;
&lt;li&gt;在 Ollama 实例中加载转换后的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;连接到云平台：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用云平台提供的 SDK（如 AWS SDK、GCP SDK）连接到 Ollama 实例。&lt;/li&gt;
&lt;li&gt;发送推理请求，运行 DeepSeek 模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;问题5-在-ollama-中运行-deepseek-模型时遇到性能问题该如何优化&#34;&gt;问题：5. 在 Ollama 中运行 DeepSeek 模型时遇到性能问题，该如何优化？
&lt;/h4&gt;&lt;h4 id=&#34;答案-4&#34;&gt;答案：
&lt;/h4&gt;&lt;p&gt;如果在 Ollama 中运行 DeepSeek 模型时遇到性能问题，可以尝试以下优化方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;模型量化：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用量化技术（如 FP16 或 INT8）减少模型大小和内存占用。&lt;/li&gt;
&lt;li&gt;化转换工具的量化设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;实例配置：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;根据模型性能需求，选择适当的实例大小（如 T4、V100 等 GPU）。&lt;/li&gt;
&lt;li&gt;调整多 GPU 实例的负载均衡策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;优化代码：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用优化的 Ollama 库或框架（如 o2c 或 ollama-cpp）。&lt;/li&gt;
&lt;li&gt;编写高效的推理代码，减少 I/O 和通信开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;监控和调整：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;使用云平台的监控工具（如 AWS CloudWatch、GCP Datadog）监控 Ollama 实例的性能。&lt;/li&gt;
&lt;li&gt;根据监控结果逐步优化模型大小、实例配置和推理代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结：
&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;部署 DeepSeek 到 Ollama 涉及模型转换、实例配置和性能优化等多个环节。确保每一步都按照正确的步骤执行，并根据具体情况调整配置，以最大化性能和效率。遇到问题时，参考官方文档和社区资源，或联系技术支持。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;big&gt;完&lt;/big&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;作者: Shanziyi &lt;br&gt;
本文纯属自己撰稿，无具体参考资料 &lt;br&gt;
©Copyright 2025 Shanziyi&lt;br&gt;
&lt;strong&gt;本文采用&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA 4.0协议&lt;/a&gt;进行署名——Shanziyi&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南</title>
        <link>https://goodjobabc.github.io/p/%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%84%89%E5%BF%AB%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-%E5%88%B0-ollama%E4%B8%80%E4%B8%AA%E8%BD%BB%E6%9D%BE%E6%8C%87%E5%8D%97/</link>
        <pubDate>Sun, 09 Feb 2025 11:00:19 +0800</pubDate>
        
        <guid>https://goodjobabc.github.io/p/%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%84%89%E5%BF%AB%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-%E5%88%B0-ollama%E4%B8%80%E4%B8%AA%E8%BD%BB%E6%9D%BE%E6%8C%87%E5%8D%97/</guid>
        <description>&lt;h1 id=&#34;在本地愉快地部署-deepseek-到-ollama一个轻松指南&#34;&gt;在本地愉快地部署 DeepSeek 到 Ollama：一个轻松指南
&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;在学习的时候配上纯音乐吧！&lt;/strong&gt;&lt;/p&gt;
&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=30987703&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;嗯，最近我在学习如何在本地部署模型，想着能不能把 DeepSeek 搬到 Ollama 里，这样用起来应该挺方便的吧！作为一个新手，我决定写一篇文章分享我的探索过程，希望能帮到像我一样的小白。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;准备阶段安装好装备&#34;&gt;准备阶段：安装好装备
&lt;/h2&gt;&lt;p&gt;首先，我需要准备好几个“装备”，这些装备是部署 DeepSeek 的必要条件：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;操作系统：当然是需要一台能流畅运行的电脑，Windows、Mac 或者 Linux 都可以。
硬件：显卡、内存、硬盘空间都需要足够的容量。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编程环境：我选择了 Python，毕竟机器学习离不开它。所以，我得先安装 Python 和一些必要的依赖库，比如 numpy 和 pandas。这些库可以通过 pip 安装：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install numpy pandas
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;深度学习框架&lt;/b&gt;：为了训练或部署模型，我需要 PyTorch 或 TensorFlow。这里我选择了 PyTorch，因为它感觉更友好一些。安装命令是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install torch torchvision
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;下载 DeepSeek 模型：资源准备&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;接下来，我需要下载 DeepSeek 的模型文件。这个模型可是个大家伙，下载的时候要注意：&lt;/p&gt;
&lt;p&gt;&lt;b&gt;选择模型大小：&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;DeepSeek 有小有大，小版本适合在本地测试，大版本则适合实际应用。我选择了中等大小的模型，大约 20GB 左右。
下载速度：别问我为什么，就是喜欢快一点。所以我选择了网速好的时间下载，大概花了我一个晚上。
存储位置：下载完成后，我不会让它孤零零地躺在硬盘里，而是用 &lt;code&gt;mkdir -p models/deepseek&lt;/code&gt; 创建了一个专门的目录，方便以后管理。&lt;b&gt;（推荐这么做）&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;架构搭建：本地服务器搭建&lt;/b&gt;
现在，我需要搭建一个本地服务器，让它能处理 DeepSeek 的请求。这里有两个选择：本地服务框架和 Ollama。我选择了 Ollama，因为它简单易用，而且我也不太想写复杂的 API。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;安装 Ollama&lt;/strong&gt;: 首先，我需要安装 Ollama。这是一个轻量级的 API，支持多种语言。安装命令是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone --recursive https://github.com/ollama/ollama-stable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; ollama-stable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python setup.py install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;b&gt;配置 Ollama：&lt;/b&gt;安装完成后，我需要告诉 Ollama 位置在哪里。这一步可以通过 ollama config 设置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama config set-model-path models/deepseek
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama config set-inference-batch-size &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;这样一来，Ollama 就知道我的模型在哪里了。&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;搭配-deepseek模型配置&#34;&gt;搭配 DeepSeek：模型配置
&lt;/h4&gt;&lt;p&gt;现在，我需要让 DeepSeek 和 Ollama 配对，这样才能协作。这一步有点挑战&lt;/p&gt;
&lt;p&gt;设置 DeepSeek 配置文件：DeepSeek 的配置文件位于 &lt;code&gt;models/deepseek/params.json 或 .toml 或 .yaml&lt;/code&gt;。我需要确保这个文件的路径正确。如果路径不对，DeepSeek 和 Ollama 就无法通信了。
启动 DeepSeek：运行 DeepSeek 后，我需要告诉它 Ollama 的服务端口。通常，Ollama 服务监听在 1234 端口。所以，我添加了以下配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;deepseek&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;basedir&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;models&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nx&#34;&gt;deepseek&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;endpoint&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;inference-batch-size&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;测试阶段验证部署&#34;&gt;测试阶段：验证部署
&lt;/h4&gt;&lt;p&gt;终于到了测试环节，我需要确保部署成功。步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;运行 Ollama 服务：启动 Ollama 服务，确保它可以正常工作。打开终端：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python -m ollama ollama-stable/ollama
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;如果服务启动成功，Ollama 会输出一条欢快的的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;访问 DeepSeek 端点：打开浏览器，进入 http://localhost:8080 。这是我配置的 Default 服务端口。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发送预测请求：输入一些简单的文本，比如“Hello, world!”，然后按回车。Ollama 应该会将请求转发给 DeepSeek，然后返回预测结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;成功时刻部署完成&#34;&gt;成功时刻：部署完成
&lt;/h4&gt;&lt;p&gt;当我看到 DeepSeek 和 Ollama 顺利通信时，内心充满了喜悦。虽然整个过程有点曲折，但最终的结果让我感到成就感满满。&lt;/p&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结
&lt;/h4&gt;&lt;p&gt;部署 DeepSeek 到本地 Ollama 的过程虽然有点复杂，但通过一步步的尝试，我成功了！这让我更加了解了机器学习模型的部署流程，也让我对 Ollama 这个平台有了更深的了解。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;希望这篇文章能帮助到和我一样对部署模型感到困惑的小白。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;big&gt;完&lt;/big&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：Shanziyi&lt;br&gt;
本文纯属自己撰稿，转载时请标明出处 ！&lt;br&gt;
© Shanziyi 2025&lt;br&gt;
彩蛋：deepseek也参与到了撰稿中！&lt;br&gt;
&lt;strong&gt;本文采用&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;CC BY-NC-SA 4.0协议&lt;/a&gt;进行署名——Shanziyi&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>
